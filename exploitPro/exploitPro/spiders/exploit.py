import datetime
import logging
import json
import scrapy
from exploitPro.items import ExploitproItem

from exploitPro import settings

# 实例化日志类
logger = logging.getLogger(__name__)


class MyDefineError(Exception):
    def __init__(self, name):
        self.name = name


class ExploitSpider(scrapy.Spider):
    name = "exploit"
    # allowed_domains = ["www.xxx.com"]
    start_urls = ["https://www.exploit-db.com/exploits/1"]
    url = 'https://www.exploit-db.com/exploits/%d'

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        settings.MYSQL_HOST = kwargs.get('host') if kwargs.get('host') else settings.MYSQL_HOST
        settings.MYSQL_USER = kwargs.get('user') if kwargs.get('user') else settings.MYSQL_USER
        settings.MYSQL_PWD = kwargs.get('pwd') if kwargs.get('pwd') else settings.MYSQL_PWD
        settings.MYSQL_PORT = kwargs.get('port') if kwargs.get('port') else settings.MYSQL_PORT
        settings.MYSQL_DB = kwargs.get('db') if kwargs.get('db') else settings.MYSQL_DB
        settings.MYSQL_TB = kwargs.get('tb') if kwargs.get('tb') else settings.MYSQL_TB
        self.page_max = int(kwargs.get('page')) if kwargs.get('page') else int(0)
        self.page = 1
        self.data_null = 0
        if self.page_max:
            self.page_max += self.page
        self.failed_requests = []
        self.database_max = 0

    def parse(self, response):
        # 导入存储在类中的数据库类 查询是否在数据库中存在
        data = self.data
        if not self.database_max:
            self.database_max = int(json.loads(data.data_max(table='exploitdata', fields='edb_id')))

        # .extract_first()
        if self.data_null >= 20:
            self.crawler.engine.close_spider(self, '空白页超过最大限制，停止程序!')

        if response.url in self.failed_requests:
            self.failed_requests.remove(response.url)

        try:
            t = {}
            title = str(
                response.xpath(
                    '/html/body/div/div[2]/div[2]/div/div/div[1]/div/div[1]/h1/text()').extract_first()).strip()
            data_list = response.xpath(
                '/html/body//div[@class="main-panel"]//div[@class="row"]//div[@class="col-sm-12 col-md-6 col-lg-3 d-flex align-items-stretch"]//div[@class="col-6 text-center"]')
            t['title'] = title
            if title == "None":
                self.data_null += 1
                raise MyDefineError('404')
            self.data_null = 0
            for data in data_list:
                key = data.xpath('.//h4/text()').extract_first().strip().replace(":", "")
                velus = ''.join(data.xpath('.//h6//text()').extract()).strip()
                t[key] = velus

            item = ExploitproItem()
            item['title'] = t['title']
            item['url'] = response.url
            item['stime'] = str(datetime.datetime.now())
            item['edb_id'] = t['EDB-ID']
            item['cve'] = "CVE-" + t['CVE']
            item['author'] = t['Author']
            item['type'] = t['Type']
            item['platform'] = t['Platform']
            item['data_time'] = t['Date']
            self.data_null = 0
            # 数据总数加1
            self.data_max += 1
            data = self.data
            logger.error(f"校验{item['url']}连接数据是否需要实例化存储")
            err = self.VerificationData(Mdata=data, item=item, response=response)
            if err:
                yield item
            else:
                logger.error(f"{item['url']}数据已是最新内容，无需实例化")

        except Exception as e:
            if type(e) == MyDefineError:
                self.max += 1
                logger.error(f"{response.url}链接无数据")
            else:
                logger.error(f"{response.url}数据获取失败,重新请求")
                yield scrapy.Request(url=response.url, callback=self.parse, meta={'download_timeout': 30})

        if self.page_max:
            self.page += 1
            if self.page <= self.page_max:
                url = format(self.url % self.page)
                yield scrapy.Request(url=url, callback=self.parse, dont_filter=True, meta={'download_timeout': 30})
        else:
            self.database_max += 1
            url = format(self.url % self.database_max)
            yield scrapy.Request(url=url, callback=self.parse, dont_filter=True, meta={'download_timeout': 30})

    def VerificationData(self, Mdata, item, response):
        """
        :param self:
        :param Mdata:
        :param item:
        :param response:
        :return:
        """
        try:
            # 校验数据是否存在数据库
            err = Mdata.price_exists(field='url', price=item['url'])
            # 不存在数据库，直接进行存储
            if err['error'] == 101:
                item['spider_order'] = 1
                return item
            # 查询语句出错，返回信息，停止程序
            elif err['error'] == 104 or err['error'] == 103:
                logger.error(f'{item["url"]}数据查询失败：错误信息{err["log"]}')
                self.crawler.engine.close_spider(self, '数据库查询信息出错')

            # 查询数据库中已有数据的数据更新时间
            data_stime = Mdata.query_page(field=item['url'], name='data_time', fields='url')[0]
            # 没有数据更新时间，再次进行存储
            if not data_stime:
                logger.error(f'{item["url"]}数据存在,但无更新日期，进行更新')
                item['spider_order'] = int(
                    Mdata.query_page(field=item['url'], name='spider_order', fields='url')[0]) + 1
                return item
            # 存在更新时间与当前爬取到时间比较，是否需要数据存储
            date_obj = datetime.datetime.strptime(item['data_time'], "%Y-%m-%d")
            # 有新的更新时间进行数据存储
            if data_stime < date_obj:
                logger.error(f'更新{item["url"]}数据')
                item['spider_order'] = int(
                    Mdata.query_page(field=item['url'], name='spider_order', fields='url')[0]) + 1
                return item

        except Exception as e:
            logger.error(f"数据库校验数据失败，错误信息{e}")
